{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task-1 Generation Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and download required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import cloudscraper\n",
    "import traceback\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/abhay/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/abhay/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt_tab to /Users/abhay/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/abhay/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloudscraper\n",
    "\n",
    "**Cloudscraper is an open-source library that bypasses and handles cloudflare protection and retreives the content of a website easily. It works on most of the websites.**\n",
    "[Cloudscraper](https://github.com/VeNoMouS/cloudscraper/tree/master)\n",
    "\n",
    "> A single instance of the cloudscraper can be used to scrape data form the websites.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = cloudscraper.create_scraper(delay=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The bottom cell is used for trial and error purpose to identify and scrape the contents from the website**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of conditions starting with 'A' 95\n",
      "number of conditions starting with 'B' 100\n",
      "/conditions/baby/\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.nhs.uk/conditions/\"\n",
    "info = scraper.get(url)\n",
    "soup = BeautifulSoup(info.content, \"html.parser\") \n",
    "total_links_list = soup.find_all('ul',{\"class\":\"nhsuk-list nhsuk-list--border\"})\n",
    "a = total_links_list[0].find_all('a')\n",
    "print(\"number of conditions starting with 'A'\",len(a))\n",
    "b = total_links_list[1].find_all('a')\n",
    "print(\"number of conditions starting with 'B'\",len(b))\n",
    "print(b[0]['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebsiteScraper:\n",
    "    \"\"\"\n",
    "    This scrapper class is constructed to obtain data from a website\n",
    "    using cloudscraper. Cloudscraper worker well with websites that have\n",
    "    cloudflare protection and anti-bot strategies\n",
    "    \n",
    "    Attributes:\n",
    "        scraper(cloudscraper.Cloudscraper): This object handles HTTP requests\n",
    "        page_nums(list): list for maintaining page numbers\n",
    "        page_links: list for maintaining urls of pages that need to be scraped\n",
    "        base_url: The base url of the website\n",
    "    \"\"\"\n",
    "    scraper = cloudscraper.create_scraper(delay=10)\n",
    "    \n",
    "    def __init__(self, base_url=\"https://www.nhs.uk\"):\n",
    "        \"\"\"\n",
    "        Initializes the instance of the WebsiteScraper with base_url\n",
    "\n",
    "        Args:\n",
    "            base_url (str, optional): The base_url of the website. Default is \"https://www.nhs.uk\".\n",
    "            page_nums(list): list for maintaining page numbers\n",
    "            page_links(list): list for maintaining urls of pages that need to be scraped\n",
    "        \"\"\"\n",
    "        self.base_url = base_url\n",
    "        #self.num_pages = num_pages\n",
    "        self.page_nums = []\n",
    "        self.page_links = []\n",
    "    \n",
    "    def scrape_page_links(self, letter_index):\n",
    "        \"\"\"\n",
    "        Scrapes and stores the urls of pages based on the letter index.\n",
    "        In this method the urls of the websites are appended to \n",
    "        self.page_links and their respective numbers are appended to \n",
    "        self.page_nums\n",
    "\n",
    "        Args:\n",
    "            letter_index (int): It is the index of the letters in alphabetical order(A->0, B->1, ...)\n",
    "        \"\"\"\n",
    "        \n",
    "        page_response = scraper.get(self.base_url+'/conditions')\n",
    "        soup = BeautifulSoup(page_response.text, 'html.parser')\n",
    "        # letter_wise_content has info about all the diseases in an alphabetical order\n",
    "        # It is a list of list. We will choose the second list as it has references to 100 diseases page links in it\n",
    "        letter_wise_content = soup.find_all('ul',{\"class\":\"nhsuk-list nhsuk-list--border\"})\n",
    "        page_number=1\n",
    "        page_refs = letter_wise_content[letter_index].find_all('a')\n",
    "        for page in page_refs:\n",
    "            page_path = page['href']\n",
    "            page_full_link = self.base_url+page_path\n",
    "            self.page_links.append(page_full_link)\n",
    "            self.page_nums.append(page_number)\n",
    "            page_number+=1\n",
    "    \n",
    "    def scrape_info(self,url):\n",
    "        \"\"\"\n",
    "        Scrapes the text from a specific website and returns it\n",
    "\n",
    "        Args:\n",
    "            url (str): The url of the website to be scraped\n",
    "\n",
    "        Returns:\n",
    "            str: It returns the scraped content from the website corresponding\n",
    "            to the url. Returns an empty string if an error occurs.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            page_response = scraper.get(url)\n",
    "            soup = BeautifulSoup(page_response.text, 'html.parser')\n",
    "            info = soup.get_text(separator=' ')\n",
    "            return info\n",
    "        except:\n",
    "            print(f\"Error occured while scraping {url}: {e}\")\n",
    "            return \"\"\n",
    "        \n",
    "    def scrape_pages(self, letter_index=1):\n",
    "        \"\"\"\n",
    "        Extracts and returns content from multiple pages corresponding to\n",
    "        a given letter index or alphabetical section(A->0, B->1)\n",
    "\n",
    "        Args:\n",
    "            letter_index (int, optional): The index of the required alphabetical section. Defaults to 1(B).\n",
    "            The default value is set to 1 because it has 100 pages of data related to various health conditions\n",
    "\n",
    "        Returns:\n",
    "            list: List containing the scraped text from the web pages. Each element in a list\n",
    "            corresponds to the scraped text of a particular page\n",
    "        \"\"\"\n",
    "        self.scrape_page_links(letter_index)\n",
    "        scraped_content = []\n",
    "        for idx, page_link in enumerate(self.page_links):\n",
    "            print(f\"scraping content for page: {idx+1}\")\n",
    "            text = self.scrape_info(page_link)\n",
    "            scraped_content.append(text)\n",
    "        return scraped_content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatisticAnalyzer:\n",
    "    \"\"\"\n",
    "    This class is used to calculate NLP related statistics like sentences_count, \n",
    "    word_count, number of nouns, number of verbs and average word length\n",
    "    \n",
    "    Attributes:\n",
    "        texts(list): List of texts on which metrics or statistics need to be calculated\n",
    "        page_numbers(list): page numbers list corresponding to each text\n",
    "        statistics(list): List to store nlp-related statistics of each text\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,texts, page_numbers ):\n",
    "        \"\"\"\n",
    "        Initializes the instance of StatisticAnalyzer with page numbers and texts\n",
    "\n",
    "        Args:\n",
    "            texts (list[str]): List of texts on which metrics or statistics need to be calculated\n",
    "            page_numbers (int): page numbers list corresponding to each text\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.page_numbers = page_numbers\n",
    "        self.statistics = []\n",
    "        \n",
    "    def analyze_each_text(self, text):\n",
    "        \"\"\"\n",
    "        Analyzes a single text string corresponding to each page to compute various statistics like\n",
    "        sentence count, word count, noun and verb count and average word length\n",
    "        \n",
    "        Args:\n",
    "            text (str): The text string that needs to be analyzed\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary of statistics\n",
    "                - num_sentences(int): Number of sentences in the text\n",
    "                - num_words(int): Number of words in the text\n",
    "                - num_verbs(int): Number of verbs in the text\n",
    "                - num_nouns(int): number of nouns in the text\n",
    "                - average_word_length(float): average length of words in the text\n",
    "                \n",
    "        \"\"\"\n",
    "        \n",
    "        sentences = sent_tokenize(text)\n",
    "        words = word_tokenize(text)\n",
    "        pos_tags = pos_tag(words)\n",
    "        \n",
    "        \n",
    "        nouns = []\n",
    "        verbs = []\n",
    "        # Findinf nouns and verbs\n",
    "        for word, tag in pos_tags:\n",
    "            if tag.startswith('NN'):\n",
    "                nouns.append(word)\n",
    "            elif tag.startswith('VB'):\n",
    "                verbs.append(word)\n",
    "                \n",
    "        # finding the average length\n",
    "        if len(words)>0:\n",
    "            avg_word_length = sum(len(word) for word in words)/len(words)\n",
    "        else:\n",
    "            avg_word_length = 0\n",
    "                \n",
    "        num_nouns = len(nouns)\n",
    "        num_verbs = len(verbs)\n",
    "        num_sentences = len(sentences)\n",
    "        num_words = len(words)\n",
    "        \n",
    "        return {\n",
    "            'num_sentences': num_sentences,\n",
    "            'num_words': num_words,\n",
    "            'num_verbs': num_verbs,\n",
    "            'num_nouns': num_nouns,\n",
    "            'average_word_length': avg_word_length\n",
    "        }\n",
    "        \n",
    "    def analyze_texts(self):\n",
    "        \"\"\"\n",
    "        Iterates over all the texts and calculates the statistics\n",
    "\n",
    "        Returns:\n",
    "            list: List of dictionaries containing page_number along with nlp-related stats for each text\n",
    "            \n",
    "        \"\"\"\n",
    "        for i, text in enumerate(self.texts):\n",
    "            print(f\"Analyzing text in page: {i+1}\")\n",
    "            each_text_stats = self.analyze_each_text(text)\n",
    "            each_text_stats['page_number'] = self.page_numbers[i]\n",
    "            self.statistics.append(each_text_stats)\n",
    "        return self.statistics\n",
    "    \n",
    "    def save_stats(self, output_file='../statistics/nlp_stats.csv'):\n",
    "        \"\"\"\n",
    "        Saves the statistics of all texts\n",
    "        Args:\n",
    "            output_file (str, optional): output file path. Defaults to '../statistics/nlp_stats.csv'.\n",
    "        \"\"\"\n",
    "        df = pd.DataFrame(self.statistics)\n",
    "        df.to_csv(output_file, index=False)\n",
    "        \n",
    "        print(\"Successfully saved statistic to statistics directory\")\n",
    "        \n",
    "    def aggregate_stats(self, output_file = '../statistics/aggregate_stats.csv' ):\n",
    "        df = pd.DataFrame(self.statistics)\n",
    "        aggregated_statistics = df.mean().to_dict()\n",
    "        aggregated_statistics_df = pd.DataFrame([aggregated_statistics])\n",
    "        aggregated_statistics_df.to_csv(output_file, index=False)\n",
    "        \n",
    "        print(\"Aggregate statistics saved to statistics directory\")\n",
    "        \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping content for page: 1\n",
      "scraping content for page: 2\n",
      "scraping content for page: 3\n",
      "scraping content for page: 4\n",
      "scraping content for page: 5\n",
      "scraping content for page: 6\n",
      "scraping content for page: 7\n",
      "scraping content for page: 8\n",
      "scraping content for page: 9\n",
      "scraping content for page: 10\n",
      "scraping content for page: 11\n",
      "scraping content for page: 12\n",
      "scraping content for page: 13\n",
      "scraping content for page: 14\n",
      "scraping content for page: 15\n",
      "scraping content for page: 16\n",
      "scraping content for page: 17\n",
      "scraping content for page: 18\n",
      "scraping content for page: 19\n",
      "scraping content for page: 20\n",
      "scraping content for page: 21\n",
      "scraping content for page: 22\n",
      "scraping content for page: 23\n",
      "scraping content for page: 24\n",
      "scraping content for page: 25\n",
      "scraping content for page: 26\n",
      "scraping content for page: 27\n",
      "scraping content for page: 28\n",
      "scraping content for page: 29\n",
      "scraping content for page: 30\n",
      "scraping content for page: 31\n",
      "scraping content for page: 32\n",
      "scraping content for page: 33\n",
      "scraping content for page: 34\n",
      "scraping content for page: 35\n",
      "scraping content for page: 36\n",
      "scraping content for page: 37\n",
      "scraping content for page: 38\n",
      "scraping content for page: 39\n",
      "scraping content for page: 40\n",
      "scraping content for page: 41\n",
      "scraping content for page: 42\n",
      "scraping content for page: 43\n",
      "scraping content for page: 44\n",
      "scraping content for page: 45\n",
      "scraping content for page: 46\n",
      "scraping content for page: 47\n",
      "scraping content for page: 48\n",
      "scraping content for page: 49\n",
      "scraping content for page: 50\n",
      "scraping content for page: 51\n",
      "scraping content for page: 52\n",
      "scraping content for page: 53\n",
      "scraping content for page: 54\n",
      "scraping content for page: 55\n",
      "scraping content for page: 56\n",
      "scraping content for page: 57\n",
      "scraping content for page: 58\n",
      "scraping content for page: 59\n",
      "scraping content for page: 60\n",
      "scraping content for page: 61\n",
      "scraping content for page: 62\n",
      "scraping content for page: 63\n",
      "scraping content for page: 64\n",
      "scraping content for page: 65\n",
      "scraping content for page: 66\n",
      "scraping content for page: 67\n",
      "scraping content for page: 68\n",
      "scraping content for page: 69\n",
      "scraping content for page: 70\n",
      "scraping content for page: 71\n",
      "scraping content for page: 72\n",
      "scraping content for page: 73\n",
      "scraping content for page: 74\n",
      "scraping content for page: 75\n",
      "scraping content for page: 76\n",
      "scraping content for page: 77\n",
      "scraping content for page: 78\n",
      "scraping content for page: 79\n",
      "scraping content for page: 80\n",
      "scraping content for page: 81\n",
      "scraping content for page: 82\n",
      "scraping content for page: 83\n",
      "scraping content for page: 84\n",
      "scraping content for page: 85\n",
      "scraping content for page: 86\n",
      "scraping content for page: 87\n",
      "scraping content for page: 88\n",
      "scraping content for page: 89\n",
      "scraping content for page: 90\n",
      "scraping content for page: 91\n",
      "scraping content for page: 92\n",
      "scraping content for page: 93\n",
      "scraping content for page: 94\n",
      "scraping content for page: 95\n",
      "scraping content for page: 96\n",
      "scraping content for page: 97\n",
      "scraping content for page: 98\n",
      "scraping content for page: 99\n",
      "scraping content for page: 100\n",
      "Analyzing text in page: 1\n",
      "Analyzing text in page: 2\n",
      "Analyzing text in page: 3\n",
      "Analyzing text in page: 4\n",
      "Analyzing text in page: 5\n",
      "Analyzing text in page: 6\n",
      "Analyzing text in page: 7\n",
      "Analyzing text in page: 8\n",
      "Analyzing text in page: 9\n",
      "Analyzing text in page: 10\n",
      "Analyzing text in page: 11\n",
      "Analyzing text in page: 12\n",
      "Analyzing text in page: 13\n",
      "Analyzing text in page: 14\n",
      "Analyzing text in page: 15\n",
      "Analyzing text in page: 16\n",
      "Analyzing text in page: 17\n",
      "Analyzing text in page: 18\n",
      "Analyzing text in page: 19\n",
      "Analyzing text in page: 20\n",
      "Analyzing text in page: 21\n",
      "Analyzing text in page: 22\n",
      "Analyzing text in page: 23\n",
      "Analyzing text in page: 24\n",
      "Analyzing text in page: 25\n",
      "Analyzing text in page: 26\n",
      "Analyzing text in page: 27\n",
      "Analyzing text in page: 28\n",
      "Analyzing text in page: 29\n",
      "Analyzing text in page: 30\n",
      "Analyzing text in page: 31\n",
      "Analyzing text in page: 32\n",
      "Analyzing text in page: 33\n",
      "Analyzing text in page: 34\n",
      "Analyzing text in page: 35\n",
      "Analyzing text in page: 36\n",
      "Analyzing text in page: 37\n",
      "Analyzing text in page: 38\n",
      "Analyzing text in page: 39\n",
      "Analyzing text in page: 40\n",
      "Analyzing text in page: 41\n",
      "Analyzing text in page: 42\n",
      "Analyzing text in page: 43\n",
      "Analyzing text in page: 44\n",
      "Analyzing text in page: 45\n",
      "Analyzing text in page: 46\n",
      "Analyzing text in page: 47\n",
      "Analyzing text in page: 48\n",
      "Analyzing text in page: 49\n",
      "Analyzing text in page: 50\n",
      "Analyzing text in page: 51\n",
      "Analyzing text in page: 52\n",
      "Analyzing text in page: 53\n",
      "Analyzing text in page: 54\n",
      "Analyzing text in page: 55\n",
      "Analyzing text in page: 56\n",
      "Analyzing text in page: 57\n",
      "Analyzing text in page: 58\n",
      "Analyzing text in page: 59\n",
      "Analyzing text in page: 60\n",
      "Analyzing text in page: 61\n",
      "Analyzing text in page: 62\n",
      "Analyzing text in page: 63\n",
      "Analyzing text in page: 64\n",
      "Analyzing text in page: 65\n",
      "Analyzing text in page: 66\n",
      "Analyzing text in page: 67\n",
      "Analyzing text in page: 68\n",
      "Analyzing text in page: 69\n",
      "Analyzing text in page: 70\n",
      "Analyzing text in page: 71\n",
      "Analyzing text in page: 72\n",
      "Analyzing text in page: 73\n",
      "Analyzing text in page: 74\n",
      "Analyzing text in page: 75\n",
      "Analyzing text in page: 76\n",
      "Analyzing text in page: 77\n",
      "Analyzing text in page: 78\n",
      "Analyzing text in page: 79\n",
      "Analyzing text in page: 80\n",
      "Analyzing text in page: 81\n",
      "Analyzing text in page: 82\n",
      "Analyzing text in page: 83\n",
      "Analyzing text in page: 84\n",
      "Analyzing text in page: 85\n",
      "Analyzing text in page: 86\n",
      "Analyzing text in page: 87\n",
      "Analyzing text in page: 88\n",
      "Analyzing text in page: 89\n",
      "Analyzing text in page: 90\n",
      "Analyzing text in page: 91\n",
      "Analyzing text in page: 92\n",
      "Analyzing text in page: 93\n",
      "Analyzing text in page: 94\n",
      "Analyzing text in page: 95\n",
      "Analyzing text in page: 96\n",
      "Analyzing text in page: 97\n",
      "Analyzing text in page: 98\n",
      "Analyzing text in page: 99\n",
      "Analyzing text in page: 100\n",
      "Successfully saved statistic to statistics directory\n",
      "Aggregate statistics saved to statistics directory\n"
     ]
    }
   ],
   "source": [
    "health_scraper = WebsiteScraper(base_url='https://www.nhs.uk')\n",
    "scraped_content = health_scraper.scrape_pages(letter_index=1)\n",
    "\n",
    "text_analyzer = StatisticAnalyzer(scraped_content, health_scraper.page_nums)\n",
    "text_analyzer.analyze_texts()\n",
    "text_analyzer.save_stats(output_file='../statistics/nlp_stats.csv')\n",
    "text_analyzer.aggregate_stats(output_file = '../statistics/aggregate_stats.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
